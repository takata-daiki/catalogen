{
  "matchterm": "Metafile.Header",
  "previouspage": null,
  "searchterm": "Metafile.Header",
  "query": "Metafile.Header",
  "language_filters": [
    {
      "count": 150,
      "id": 23,
      "language": "Java"
    }
  ],
  "total": 150,
  "results": [
    {
      "repo": "https://github.com/minstrelsy/POI-Android.git",
      "language": "Java",
      "linescount": 196,
      "location": "/ppt/scratchpad/src/org/apache/poi/hslf/blip",
      "name": "POI-Android",
      "url": "https://searchcode.com/codesearch/view/97394516/",
      "md5hash": "d0a7dfe6144b031234382ee06549b69e",
      "lines": {
        "33": " */",
        "34": "public final class WMF extends Metafile {",
        "48": "            InputStream is = new ByteArrayInputStream( rawdata );",
        "49": "            Header header = new Header();",
        "50": "            header.read(rawdata, CHECKSUM_SIZE);",
        "51": "            is.skip(header.getSize() + CHECKSUM_SIZE);",
        "52": "",
        "53": "            AldusHeader aldus = new AldusHeader();",
        "54": "            aldus.left = header.bounds.x;",
        "55": "            aldus.top = header.bounds.y;",
        "56": "            aldus.right = header.bounds.x + header.bounds.width;",
        "57": "            aldus.bottom = header.bounds.y + header.bounds.height;",
        "58": "            aldus.write(out);",
        "29": "/**",
        "30": " * Represents a WMF (Windows Metafile) picture data."
      },
      "id": 97394516,
      "filename": "WMF.java"
    },
    {
      "repo": "https://github.com/bmorrise/pentaho-reporting.git",
      "language": "Java",
      "linescount": 276,
      "location": "/libraries/libpixie/source/org/pentaho/reporting/libraries/pixie/wmf",
      "name": "pentaho-reporting",
      "url": "https://searchcode.com/codesearch/view/100684728/",
      "md5hash": "1a2fc7a48b21804db325227fac3d7566",
      "lines": {
        "59": "   * and the extended wmf header; the standard header is always placed after the extended",
        "38": " */",
        "39": "public class MfHeader extends Buffer",
        "60": "   * header.",
        "56": "",
        "58": "   * Metadata Positions This implementation always reserves space for both the standard",
        "53": "",
        "54": "  private static final int PLACEABLE_HEADER_SIZE = 22;",
        "55": "  private static final int STANDARD_HEADER_SIZE = 18;",
        "24": "/**",
        "25": " * A buffer which represents a Metafile header.",
        "26": " * <p/>",
        "27": " * The meta file header has the following structure <table border=\"1\"> <tr>",
        "28": " * <th>offset</th> <th>length in bytes</th> <th>name</th> <th>meaning</th> </tr> <tr>",
        "29": " * <td>0x00</td> <td>2</td> <td>mfType</td> <td>MetaFile type: 0x1 = memory based meta",
        "30": " * file, 0x2 = disk based meta file</td> </tr> <tr> <td>0x02</td> <td>2</td>",
        "31": " * <td>mfHeader</td> <td>length of header in words (16bit)</td> </tr> <tr> <td>0x04</td>"
      },
      "id": 100684728,
      "filename": "MfHeader.java"
    },
    {
      "repo": "https://github.com/jboulon/hadoop-20.git",
      "language": "Java",
      "linescount": 1027,
      "location": "/src/hdfs/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-20",
      "url": "https://searchcode.com/codesearch/view/74231870/",
      "md5hash": "59e00deecbca3c79b491a7a1bb858e37",
      "lines": {
        "224": "    header.readFields(in);",
        "38": "import org.apache.hadoop.hdfs.protocol.Block;",
        "39": "import org.apache.hadoop.hdfs.protocol.BlockChecksumHeader;",
        "40": "import org.apache.hadoop.hdfs.protocol.CopyBlockHeader;",
        "41": "import org.apache.hadoop.hdfs.protocol.ReadMetadataHeader;",
        "42": "import org.apache.hadoop.hdfs.protocol.ReplaceBlockHeader;",
        "43": "import org.apache.hadoop.hdfs.protocol.VersionAndOpcode;",
        "46": "import org.apache.hadoop.hdfs.protocol.FSConstants;",
        "47": "import org.apache.hadoop.hdfs.protocol.ReadBlockHeader;",
        "48": "import org.apache.hadoop.hdfs.protocol.WriteBlockHeader;",
        "49": "import org.apache.hadoop.hdfs.server.common.HdfsConstants;",
        "218": "    //",
        "219": "    // Read in the header",
        "222": "    ",
        "223": "    ReadBlockHeader header = new ReadBlockHeader(versionAndOpcode);"
      },
      "id": 74231870,
      "filename": "DataXceiver.java"
    },
    {
      "repo": "https://bitbucket.org/ivertex/itext-fork",
      "language": "Java",
      "linescount": 762,
      "location": "/src/core/com/itextpdf/text/pdf/codec/wmf",
      "name": "itext-fork",
      "url": "https://searchcode.com/codesearch/view/57657496/",
      "md5hash": "4ff08bdb50c8e2fec2c0b2c3780037bd",
      "lines": {
        "147": "        if (in.readInt() != 0x9AC6CDD7) {",
        "692": "        ByteArrayOutputStream os = new ByteArrayOutputStream();",
        "693": "        // write metafile header",
        "697": "        writeDWord(os, 9 + 4 + 5 + 5 + (13 + sizeBmpWords) + 3); // total metafile size",
        "696": "        writeWord(os, 0x0300);",
        "148": "            throw new DocumentException(MessageLocalization.getComposedMessage(\"not.a.placeable.windows.metafile\"));"
      },
      "id": 57657496,
      "filename": "MetaDo.java"
    },
    {
      "repo": "https://github.com/faquino/iText-4.2.0.git",
      "language": "Java",
      "linescount": 770,
      "location": "/src/core/com/lowagie/text/pdf/codec/wmf",
      "name": "iText-4.2.0",
      "url": "https://searchcode.com/codesearch/view/70298690/",
      "md5hash": "783ce72c544c492cc1941a79f8fe2521",
      "lines": {
        "704": "        writeWord(os, 0x0300);",
        "705": "        writeDWord(os, 9 + 4 + 5 + 5 + (13 + sizeBmpWords) + 3); // total metafile size",
        "156": "            throw new DocumentException(MessageLocalization.getComposedMessage(\"not.a.placeable.windows.metafile\"));",
        "155": "        if (in.readInt() != 0x9AC6CDD7) {",
        "700": "        ByteArrayOutputStream os = new ByteArrayOutputStream();",
        "701": "        // write metafile header"
      },
      "id": 70298690,
      "filename": "MetaDo.java"
    },
    {
      "repo": "https://bitbucket.org/keithb/tdd",
      "language": "Java",
      "linescount": 768,
      "location": "/targets/itext/1.4.8/com/lowagie/text/pdf/codec/wmf",
      "name": "tdd",
      "url": "https://searchcode.com/codesearch/view/120313899/",
      "md5hash": "e4c1ef18ba9824f1bad4318bd5c2f6e6",
      "lines": {
        "704": "        writeDWord(os, 9 + 4 + 5 + 5 + (13 + sizeBmpWords) + 3); // total metafile size",
        "699": "        ByteArrayOutputStream os = new ByteArrayOutputStream();",
        "154": "        if (in.readInt() != 0x9AC6CDD7) {",
        "155": "            throw new DocumentException(\"Not a placeable windows metafile\");",
        "700": "        // write metafile header",
        "703": "        writeWord(os, 0x0300);"
      },
      "id": 120313899,
      "filename": "MetaDo.java"
    },
    {
      "repo": "http://fb2pdf.googlecode.com/svn/trunk/",
      "language": "Java",
      "linescount": 762,
      "location": "/src/java/src/com/itextpdf/text/pdf/codec/wmf",
      "name": "fb2pdf",
      "url": "https://searchcode.com/codesearch/view/2127028/",
      "md5hash": "9814e885924955f872b23c2933b63500",
      "lines": {
        "147": "        if (in.readInt() != 0x9AC6CDD7) {",
        "692": "        ByteArrayOutputStream os = new ByteArrayOutputStream();",
        "693": "        // write metafile header",
        "697": "        writeDWord(os, 9 + 4 + 5 + 5 + 13 + sizeBmpWords + 3); // total metafile size",
        "696": "        writeWord(os, 0x0300);",
        "148": "            throw new DocumentException(MessageLocalization.getComposedMessage(\"not.a.placeable.windows.metafile\"));"
      },
      "id": 2127028,
      "filename": "MetaDo.java"
    },
    {
      "repo": "https://bitbucket.org/tjklemz/itextmod",
      "language": "Java",
      "linescount": 762,
      "location": "/src/com/itextpdf/text/pdf/codec/wmf",
      "name": "itextmod",
      "url": "https://searchcode.com/codesearch/view/125285923/",
      "md5hash": "59ef9b2bc5f2b822fe21ee501fc0d5c8",
      "lines": {
        "147": "        if (in.readInt() != 0x9AC6CDD7) {",
        "692": "        ByteArrayOutputStream os = new ByteArrayOutputStream();",
        "693": "        // write metafile header",
        "697": "        writeDWord(os, 9 + 4 + 5 + 5 + 13 + sizeBmpWords + 3); // total metafile size",
        "696": "        writeWord(os, 0x0300);",
        "148": "            throw new DocumentException(MessageLocalization.getComposedMessage(\"not.a.placeable.windows.metafile\"));"
      },
      "id": 125285923,
      "filename": "MetaDo.java"
    },
    {
      "repo": "https://github.com/jboulon/hadoop-20.git",
      "language": "Java",
      "linescount": 450,
      "location": "/src/hdfs/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-20",
      "url": "https://searchcode.com/codesearch/view/74231744/",
      "md5hash": "66e68d9ccc7058ffc95cd414bbef94fa",
      "lines": {
        "368": "   */",
        "66": " *  +----------------------+",
        "67": " *  |   Checksum Header    |",
        "369": "  static long parseGenerationStampInMetaFile(File blockFile, File metaFile",
        "400": "      Block b) throws IOException {",
        "398": "  ",
        "399": "  static protected File getMetaFile(FSDatasetInterface dataset, int namespaceId,",
        "112": "      short version = header.getVersion();",
        "113": "",
        "370": "      ) throws IOException {",
        "401": "    return BlockWithChecksumFileWriter.getMetaFile(dataset.getBlockFile(namespaceId, b), b);",
        "407": "   * @param b - the block",
        "408": "   * @return true of the metafile for specified block exits",
        "410": "   */",
        "111": "      BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);",
        "411": "  static public boolean metaFileExists(FSDatasetInterface dataset, int namespaceId, Block b) throws IOException {"
      },
      "id": 74231744,
      "filename": "BlockWithChecksumFileReader.java"
    },
    {
      "repo": "https://github.com/jayanth1991/hadoop-hdfs.git",
      "language": "Java",
      "linescount": 2136,
      "location": "/src/java/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-hdfs",
      "url": "https://searchcode.com/codesearch/view/99685507/",
      "md5hash": "dba364a38fe18cfe8558c7c5810d853e",
      "lines": {
        "192": "                        \" does not have a metafile!\");",
        "480": "        long blockFileLen = blockFile.length();",
        "482": "        int crcHeaderLen = DataChecksum.getChecksumHeaderSize();",
        "483": "        if (!blockFile.exists() || blockFileLen == 0 ||",
        "484": "            !metaFile.exists() || metaFileLen < (long)crcHeaderLen) {",
        "481": "        long metaFileLen = metaFile.length();",
        "191": "      DataNode.LOG.warn(\"Block \" + blockFile + ",
        "176": "    /** Find the metadata file for the specified block file.",
        "177": "     * Return the generation stamp from the name of the metafile.",
        "410": "      File blockFile = dataDir.addBlock(b, f);",
        "411": "      File metaFile = getMetaFile( blockFile , b);",
        "412": "      dfsUsage.incDfsUsed(b.getNumBytes()+metaFile.length());",
        "413": "      return blockFile;",
        "478": "      try {",
        "479": "        File metaFile = new File(getMetaFileName(blockFile.toString(), genStamp));"
      },
      "id": 99685507,
      "filename": "FSDataset.java"
    },
    {
      "repo": "https://github.com/jboulon/hadoop-20.git",
      "language": "Java",
      "linescount": 460,
      "location": "/src/hdfs/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-20",
      "url": "https://searchcode.com/codesearch/view/74231786/",
      "md5hash": "565a80c07fcae563b372db3023851931",
      "lines": {
        "226": "        + \" block \" + block + \" offset in block \" + blkoff",
        "227": "        + \" offset in metafile \" + ckoff);",
        "324": "      }",
        "325": "      // update metaFile",
        "326": "      RandomAccessFile metaRAF = new RandomAccessFile(metafile, \"rw\");",
        "327": "      try {",
        "328": "        metaRAF.setLength(BlockMetadataHeader.getHeaderSize());",
        "333": "    }",
        "78": "public class BlockWithChecksumFileWriter extends DatanodeBlockWriter {",
        "79": "  File blockFile, metafile;",
        "334": "    DataChecksum dcs = BlockMetadataHeader.readHeader(metafile).getChecksum();",
        "152": "    }",
        "153": "    long offsetInChecksum = BlockMetadataHeader.getHeaderSize() + offsetInBlock",
        "62": " *  +----------------------+   ",
        "63": " *  |   Checksum Header    |   "
      },
      "id": 74231786,
      "filename": "BlockWithChecksumFileWriter.java"
    },
    {
      "repo": "https://github.com/lihuibng/hadoop-20.git",
      "language": "Java",
      "linescount": 3105,
      "location": "/src/hdfs/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-20",
      "url": "https://searchcode.com/codesearch/view/75370514/",
      "md5hash": "4eb5cc7b7a3bc80b128f2a6f9a885c9d",
      "lines": {
        "1858": "  ",
        "1859": "  /** Header size for a packet */",
        "1860": "  public static int PKT_HEADER_LEN = ( 4 + /* Packet payload length */",
        "1831": "",
        "1832": "    PACKET : Contains a packet header, checksum and data. Amount of data",
        "2444": "    BlockPathInfo info = new BlockPathInfo(block, datafile.getAbsolutePath(), ",
        "1930": "        //",
        "1835": "      +-----------------------------------------------------+",
        "1836": "      | 4 byte packet length (excluding packet header)      |",
        "2445": "                                           metafile.getAbsolutePath());",
        "2448": "                \" blockfile \" + datafile.getAbsolutePath() +",
        "2449": "                \" metafile \" + metafile.getAbsolutePath());",
        "1811": "",
        "1812": "      ChecksumHeader :",
        "2443": "    File metafile = FSDataset.getMetaFile(datafile, block);",
        "2442": "    File datafile = data.getBlockFile(namespaceId, block);",
        "1931": "        // Header info"
      },
      "id": 75370514,
      "filename": "DataNode.java"
    },
    {
      "repo": "https://github.com/platy/fred-staging.git",
      "language": "Java",
      "linescount": 1840,
      "location": "/src/freenet/store/saltedhash",
      "name": "fred-staging",
      "url": "https://searchcode.com/codesearch/view/105062378/",
      "md5hash": "2f210baf28c18746946203ccb0d13d23",
      "lines": {
        "354": "\t\t\t\t\t// Overwrite old offset with same key",
        "355": "\t\t\t\t\tEntry entry = new Entry(routingKey, header, data, !isOldBlock);",
        "453": "\t\tprivate int generation;",
        "454": "\t\tbyte[] header;",
        "401": "\tprivate File metaFile;",
        "489": "\t\t/**",
        "362": "",
        "363": "\t\t\t\tEntry entry = new Entry(routingKey, header, data, !isOldBlock);",
        "400": "\t// meta-data file",
        "305": "",
        "306": "\tpublic void put(StorableBlock block, byte[] data, byte[] header, boolean overwrite, boolean isOldBlock) throws IOException, KeyCollisionException {",
        "403": "\tprivate FileChannel metaFC;",
        "404": "\t// header+data file",
        "87": "\tprivate final boolean collisionPossible;",
        "88": "\tprivate final int headerBlockLength;",
        "490": "\t\t * Set header/data after construction."
      },
      "id": 105062378,
      "filename": "SaltedHashFreenetStore.java"
    },
    {
      "repo": "https://github.com/bmorrise/pentaho-reporting.git",
      "language": "Java",
      "linescount": 565,
      "location": "/libraries/libpixie/source/org/pentaho/reporting/libraries/pixie/wmf",
      "name": "pentaho-reporting",
      "url": "https://searchcode.com/codesearch/view/100684759/",
      "md5hash": "1d1ad829a7e0f00408add9a6761c12a9",
      "lines": {
        "70": "  private InputStream in;",
        "71": "  private MfHeader header;",
        "141": "  /**",
        "142": "   * Initialize metafile for reading from the given input stream.",
        "207": "  /**",
        "208": "   * Read Placeable and Windows headers.",
        "209": "   */",
        "181": "  /**",
        "182": "   * Return Placeable and Windows headers that were read earlier.",
        "183": "   *",
        "184": "   * @return the meta-file header.",
        "185": "   */",
        "186": "  public MfHeader getHeader()",
        "187": "  {",
        "188": "    return header;",
        "158": "    palette = new MfPalette();",
        "159": "    readHeader();"
      },
      "id": 100684759,
      "filename": "WmfFile.java"
    },
    {
      "repo": "https://bitbucket.org/hadoop/hadoop-0.21.0",
      "language": "Java",
      "linescount": 2107,
      "location": "/hadoop-hdfs/src/java/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-0.21.0",
      "url": "https://searchcode.com/codesearch/view/57175450/",
      "md5hash": "1ba76158bc179dcac6564797c01ada33",
      "lines": {
        "128": "        File dest = new File(dir, b.getBlockName());",
        "129": "        File metaData = getMetaFile( src, b );",
        "130": "        File newmeta = getMetaFile(dest, b);",
        "131": "        if ( ! metaData.renameTo( newmeta ) ||",
        "481": "            !metaFile.exists() || metaFileLen < (long)crcHeaderLen) {",
        "499": "            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, ",
        "500": "            (metaFileLen - crcHeaderLen)/checksumSize);",
        "480": "        if (!blockFile.exists() || blockFileLen == 0 ||",
        "407": "      File blockFile = dataDir.addBlock(b, f);",
        "408": "      File metaFile = getMetaFile( blockFile , b);",
        "409": "      dfsUsage.incDfsUsed(b.getNumBytes()+metaFile.length());",
        "410": "      return blockFile;",
        "475": "      try {",
        "476": "        File metaFile = new File(getMetaFileName(blockFile.toString(), genStamp));",
        "477": "        long blockFileLen = blockFile.length();"
      },
      "id": 57175450,
      "filename": "FSDataset.java"
    },
    {
      "repo": "https://github.com/gsastry/hadoop-hdfs.git",
      "language": "Java",
      "linescount": 2169,
      "location": "/src/java/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-hdfs",
      "url": "https://searchcode.com/codesearch/view/65453984/",
      "md5hash": "077399dadbcd95c1b95c24b5af1b1db9",
      "lines": {
        "480": "      try {",
        "129": "        File dest = new File(dir, b.getBlockName());",
        "130": "        File metaData = getMetaFile( src, b );",
        "131": "        File newmeta = getMetaFile(dest, b);",
        "132": "        if ( ! metaData.renameTo( newmeta ) ||",
        "485": "        if (!blockFile.exists() || blockFileLen == 0 ||",
        "486": "            !metaFile.exists() || metaFileLen < (long)crcHeaderLen) {",
        "481": "        File metaFile = new File(getMetaFileName(blockFile.toString(), genStamp));",
        "482": "        long blockFileLen = blockFile.length();",
        "504": "            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, ",
        "505": "            (metaFileLen - crcHeaderLen)/checksumSize);",
        "412": "      File blockFile = dataDir.addBlock(b, f);",
        "413": "      File metaFile = getMetaFile( blockFile , b);",
        "414": "      dfsUsage.incDfsUsed(b.getNumBytes()+metaFile.length());",
        "415": "      return blockFile;"
      },
      "id": 65453984,
      "filename": "FSDataset.java"
    },
    {
      "repo": "git://github.com/apache/hadoop-hdfs.git",
      "language": "Java",
      "linescount": 2637,
      "location": "/src/java/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-hdfs",
      "url": "https://searchcode.com/codesearch/view/3194838/",
      "md5hash": "50d732420db33e2b07eb1852760a8198",
      "lines": {
        "973": "  static String getMetaFileName(String blockFileName, long genStamp) {",
        "489": "            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, ",
        "490": "            (metaFileLen - crcHeaderLen)/checksumSize);",
        "683": "      // TODO move this up",
        "684": "      // dfsUsage.incDfsUsed(b.getNumBytes()+metaFile.length());",
        "397": "      File blockFile = finalizedDir.addBlock(b, f);",
        "398": "      File metaFile = getMetaFile(blockFile , b.getGenerationStamp());",
        "399": "      dfsUsage.incDfsUsed(b.getNumBytes()+metaFile.length());",
        "400": "      return blockFile;",
        "465": "      try {",
        "466": "        File metaFile = new File(getMetaFileName(blockFile.toString(), genStamp));",
        "467": "        long blockFileLen = blockFile.length();",
        "470": "        if (!blockFile.exists() || blockFileLen == 0 ||",
        "471": "            !metaFile.exists() || metaFileLen < (long)crcHeaderLen) {",
        "972": "  "
      },
      "id": 3194838,
      "filename": "FSDataset.java"
    },
    {
      "repo": "https://bitbucket.org/openxls/openxls.git",
      "language": "Java",
      "linescount": 260,
      "location": "/src/main/java/com/extentech/formats/escher",
      "name": "openxls",
      "url": "https://searchcode.com/codesearch/view/47124722/",
      "md5hash": "f5192ef8cad30dfcf9ba77c4997577a3",
      "lines": {
        "64": "\t\t",
        "65": "\t\timgHeader[18] = (byte)-1;   //First tag byte is always -1",
        "66": "\t\timgHeader[19] = (byte)0;    //Second tag byte is always 0",
        "67": "\t    ",
        "69": "\t\tint mod = (imageData.length+25)%MAXROWS_BIFF8;",
        "70": "\t\timgHeader[20] = (byte)((0x000000FF&mod));    ",
        "71": "\t\timgHeader[21] = (byte)((0x0000FF00&mod)>>8);   ",
        "72": "\t\t",
        "46": "\tprotected byte[] getData(){",
        "47": "\t\tbyte[] imgHeader = new byte[61];\t// BSE header = 36 bytes, BLIP record header = 24 bytes, then imageData bytes follow ",
        "48": "\t\timgHeader[0] = (byte)imageType;   //btWin32",
        "49": "\t\timgHeader[1] = (byte)imageType;   //btMac",
        "50": "",
        "62": "\t    byte[] digest = md4Digest.getDigest(imageData);",
        "63": "\t    System.arraycopy(digest, 0, imgHeader, 2, 16);"
      },
      "id": 47124722,
      "filename": "MsofbtBSE.java"
    },
    {
      "repo": "https://bitbucket.org/ghu/openxls.git",
      "language": "Java",
      "linescount": 260,
      "location": "/src/main/java/com/extentech/formats/escher",
      "name": "openxls",
      "url": "https://searchcode.com/codesearch/view/50976346/",
      "md5hash": "f5192ef8cad30dfcf9ba77c4997577a3",
      "lines": {
        "64": "\t\t",
        "65": "\t\timgHeader[18] = (byte)-1;   //First tag byte is always -1",
        "66": "\t\timgHeader[19] = (byte)0;    //Second tag byte is always 0",
        "67": "\t    ",
        "69": "\t\tint mod = (imageData.length+25)%MAXROWS_BIFF8;",
        "70": "\t\timgHeader[20] = (byte)((0x000000FF&mod));    ",
        "71": "\t\timgHeader[21] = (byte)((0x0000FF00&mod)>>8);   ",
        "72": "\t\t",
        "46": "\tprotected byte[] getData(){",
        "47": "\t\tbyte[] imgHeader = new byte[61];\t// BSE header = 36 bytes, BLIP record header = 24 bytes, then imageData bytes follow ",
        "48": "\t\timgHeader[0] = (byte)imageType;   //btWin32",
        "49": "\t\timgHeader[1] = (byte)imageType;   //btMac",
        "50": "",
        "62": "\t    byte[] digest = md4Digest.getDigest(imageData);",
        "63": "\t    System.arraycopy(digest, 0, imgHeader, 2, 16);"
      },
      "id": 50976346,
      "filename": "MsofbtBSE.java"
    },
    {
      "repo": "git://github.com/apache/hadoop-common.git",
      "language": "Java",
      "linescount": 2671,
      "location": "/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode",
      "name": "hadoop-common",
      "url": "https://searchcode.com/codesearch/view/8197046/",
      "md5hash": "869ea20b5cfdf01612543da27edc39c2",
      "lines": {
        "481": "            (blockFileLen + bytesPerChecksum - 1)/bytesPerChecksum, ",
        "482": "            (metaFileLen - crcHeaderLen)/checksumSize);",
        "675": "      // TODO move this up",
        "676": "      // dfsUsage.incDfsUsed(b.getNumBytes()+metaFile.length());",
        "389": "      File blockFile = finalizedDir.addBlock(b, f);",
        "390": "      File metaFile = getMetaFile(blockFile , b.getGenerationStamp());",
        "391": "      dfsUsage.incDfsUsed(b.getNumBytes()+metaFile.length());",
        "392": "      return blockFile;",
        "457": "      try {",
        "458": "        File metaFile = new File(getMetaFileName(blockFile.toString(), genStamp));",
        "459": "        long blockFileLen = blockFile.length();",
        "460": "        long metaFileLen = metaFile.length();",
        "461": "        int crcHeaderLen = DataChecksum.getChecksumHeaderSize();",
        "462": "        if (!blockFile.exists() || blockFileLen == 0 ||",
        "463": "            !metaFile.exists() || metaFileLen < crcHeaderLen) {"
      },
      "id": 8197046,
      "filename": "FSDataset.java"
    }
  ],
  "page": 0,
  "nextpage": 1,
  "source_filters": [
    {
      "count": 91,
      "source": "Bitbucket",
      "id": 3
    },
    {
      "count": 56,
      "source": "Github",
      "id": 2
    },
    {
      "count": 3,
      "source": "Google Code",
      "id": 1
    }
  ]
}